{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc09d16c",
   "metadata": {},
   "source": [
    "## Import and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b31447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install segmentation-models-pytorch albumentations opencv-python pyyaml tqdm matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64148fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMAGE_SIZE = 1024\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d24c9f",
   "metadata": {},
   "source": [
    "## Define the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a978c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CensorshipDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))\n",
    "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
    "        self.transform = transform\n",
    "        self.aug = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),   \n",
    "            A.RandomRotate90(p=0.5),\n",
    "            # A.Normalize(),          \n",
    "            ToTensorV2()              \n",
    "        ])\n",
    "        self.resize = A.Resize(IMAGE_SIZE, IMAGE_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # Convert to binary mask (0 for background, 1 for censorship)\n",
    "        mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "        # Resize both before transform\n",
    "        resized = self.resize(image=image, mask=mask)\n",
    "        image, mask = resized[\"image\"], resized[\"mask\"]\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.aug(image=image, mask=mask)\n",
    "            image, mask = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        return image, mask.long()\n",
    "    \n",
    "    # Define paths for different censorship types\n",
    "dataset_base = \"dataset\"\n",
    "censorship_types = [\"black_bars\", \"white_bars\", \"transparent_black\"]\n",
    "\n",
    "# Create dictionary of datasets\n",
    "datasets = {}\n",
    "for ctype in censorship_types:\n",
    "    image_dir = os.path.join(dataset_base, \"images\", ctype)\n",
    "    mask_dir = os.path.join(dataset_base, \"masks\", ctype)\n",
    "    \n",
    "    if not os.path.exists(image_dir) or not os.path.exists(mask_dir):\n",
    "        print(f\"Warning: {ctype} directories not found, skipping.\")\n",
    "        continue\n",
    "        \n",
    "    datasets[ctype] = CensorshipDataset(image_dir, mask_dir)\n",
    "    print(f\"Loaded {ctype} dataset with {len(datasets[ctype])} images\")\n",
    "\n",
    "    print(f\"Displaying 3 samples from {ctype} dataset:\")\n",
    "    for i in range(min(3, len(datasets[ctype]))):\n",
    "        image, mask = datasets[ctype][i]\n",
    "        image = image.permute(1, 2, 0).cpu().numpy() \n",
    "        mask = mask.cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"{ctype} - Image {i+1}\")\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(mask, cmap=\"gray\")\n",
    "        plt.title(f\"{ctype} - Mask {i+1}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed4f90",
   "metadata": {},
   "source": [
    "## Create and Train Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create a binary segmentation model\"\"\"\n",
    "    return smp.UnetPlusPlus(\n",
    "        encoder_name=\"efficientnet-b6\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        encoder_depth=5,\n",
    "        in_channels=3,\n",
    "        classes=1\n",
    "    ).to(DEVICE)\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "def train_model(model, dataset, model_name, epochs=30, val_split=0.2, patience=10):\n",
    "    \"\"\"\n",
    "    Train a model on the given dataset with validation, early stopping, and learning rate scheduling\n",
    "    \n",
    "    Args:\n",
    "        model: The segmentation model to train\n",
    "        dataset: Dataset containing training images and masks\n",
    "        model_name: Name of the model (used for saving)\n",
    "        epochs: Maximum number of training epochs\n",
    "        val_split: Proportion of data to use for validation\n",
    "        patience: Number of epochs to wait before early stopping\n",
    "    \"\"\"\n",
    "    # Create validation split\n",
    "    dataset_size = len(dataset)\n",
    "    val_size = int(dataset_size * val_split)\n",
    "    train_size = dataset_size - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "    \n",
    "    # Define loss function, optimizer and scheduler\n",
    "    loss_fn = smp.losses.FocalLoss(mode='binary')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5)\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_ious = []\n",
    "    best_epoch = 0\n",
    "    \n",
    "    def iou_score(pred, target):\n",
    "        \"\"\"Calculate IoU score between prediction and target for binary segmentation\"\"\"\n",
    "        batch_size = pred.size(0)\n",
    "        total_iou = 0.0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            pred_mask = (pred[i].sigmoid() > 0.5).float()\n",
    "            target_mask = target[i].float()\n",
    "            \n",
    "            pred_mask = pred_mask.cpu()\n",
    "            target_mask = target_mask.cpu()\n",
    "            \n",
    "            intersection = (pred_mask * target_mask).sum().item()\n",
    "            union = (pred_mask + target_mask).gt(0).sum().item()\n",
    "            \n",
    "            if union == 0:\n",
    "                total_iou += 1.0  # If both prediction and target are empty, IoU is 1\n",
    "            else:\n",
    "                total_iou += intersection / union\n",
    "        \n",
    "        return total_iou / batch_size\n",
    "    \n",
    "    print(f\"Starting training for {model_name}:\")\n",
    "    print(f\"- Training samples: {train_size}\")\n",
    "    print(f\"- Validation samples: {val_size}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        \n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            preds = model(images)\n",
    "            loss = loss_fn(preds, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "                \n",
    "                # Forward pass\n",
    "                preds = model(images)\n",
    "                val_loss = loss_fn(preds, masks)\n",
    "                \n",
    "                # Debug prints (comment out after debugging)\n",
    "                if epoch == 0:  # Print only in first epoch\n",
    "                    print(f\"\\nDebug Information:\")\n",
    "                    print(f\"Predictions range: ({preds.min():.4f}, {preds.max():.4f})\")\n",
    "                    print(f\"Predictions shape: {preds.shape}\")\n",
    "                    print(f\"Masks range: ({masks.min():.4f}, {masks.max():.4f})\")\n",
    "                    print(f\"Masks shape: {masks.shape}\")\n",
    "                    print(f\"Unique mask values: {torch.unique(masks).tolist()}\")\n",
    "                \n",
    "                # Calculate metrics\n",
    "                batch_iou = iou_score(preds, masks)\n",
    "                \n",
    "                running_val_loss += val_loss.item()\n",
    "                running_iou += batch_iou\n",
    "        \n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        avg_iou = running_iou / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_ious.append(avg_iou)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\n{model_name} - Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}, IoU: {avg_iou:.4f}\")\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"  Current LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': best_val_loss,\n",
    "                'val_iou': avg_iou\n",
    "            }, f'best_{model_name}_model.pth')\n",
    "            print(f\"  Saved new best model with Val Loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_iou': avg_iou\n",
    "    }, f'final_{model_name}_model.pth')\n",
    "    \n",
    "    print(f\"Training completed. Best model saved at epoch {best_epoch} with Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} - Loss Curves')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_ious, label='Validation IoU')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('IoU Score')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_name} - IoU Curve')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_training_curves.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Load and return best model\n",
    "    best_model = create_model()\n",
    "    checkpoint = torch.load(f'best_{model_name}_model.pth')\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return best_model\n",
    "\n",
    "def load_existing_model(model_name):\n",
    "    \"\"\"\n",
    "    Try to load an existing model with the given name\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to load (e.g., 'black_bars')\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded model or None if no model file exists\n",
    "    \"\"\"\n",
    "    # Check for different possible model file paths\n",
    "    model_paths = [\n",
    "        f'best_{model_name}_model.pth',\n",
    "        f'final_{model_name}_model.pth',\n",
    "        f'segmentation_model_{model_name}.pth'\n",
    "    ]\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Found existing model at {model_path}\")\n",
    "            model = create_model()\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.eval()  # Set to evaluation mode\n",
    "            return model\n",
    "    \n",
    "    print(f\"No existing model found for {model_name}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768d5f8",
   "metadata": {},
   "source": [
    "## Train Model for Black Bars Detection\n",
    "\n",
    "This section focuses on training a model specifically for detecting black bars in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for black bars detection\n",
    "black_bars_dataset = datasets.get('black_bars')\n",
    "if black_bars_dataset:\n",
    "    print('Processing black bars model...')\n",
    "    \n",
    "    # Try to load existing model first\n",
    "    black_bars_model = load_existing_model('black_bars')\n",
    "    \n",
    "    # If no model was found, train a new one\n",
    "    if black_bars_model is None:\n",
    "        print('Training new model for black bars...')\n",
    "        black_bars_model = create_model()\n",
    "        black_bars_model = train_model(black_bars_model, black_bars_dataset, 'black_bars')\n",
    "    else:\n",
    "        print('Using existing black bars model')\n",
    "        black_bars_model.eval()\n",
    "        black_bars_model = train_model(black_bars_model, black_bars_dataset, 'black_bars', epochs=10)\n",
    "    \n",
    "    # Store in trained_models dictionary\n",
    "    trained_models = {} if 'trained_models' not in globals() else trained_models\n",
    "    trained_models['black_bars'] = black_bars_model\n",
    "else:\n",
    "    print('Black bars dataset not found. Skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73475312",
   "metadata": {},
   "source": [
    "## Train Model for White Bars Detection\n",
    "\n",
    "This section will be used to train a model for detecting white bars in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becefc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for white bars detection\n",
    "white_bars_dataset = datasets.get('white_bars')\n",
    "if white_bars_dataset:\n",
    "    print('Processing white bars model...')\n",
    "    \n",
    "    # Try to load existing model first\n",
    "    white_bars_model = load_existing_model('white_bars')\n",
    "    \n",
    "    # If no model was found, train a new one\n",
    "    if white_bars_model is None:\n",
    "        print('Training new model for white bars...')\n",
    "        white_bars_model = create_model()\n",
    "        white_bars_model = train_model(white_bars_model, white_bars_dataset, 'white_bars')\n",
    "        black_bars_model = train_model(black_bars_model, black_bars_dataset, 'black_bars', epochs=10)\n",
    "    else:\n",
    "        print('Using existing white bars model')\n",
    "    \n",
    "    # Store in trained_models dictionary\n",
    "    trained_models = {} if 'trained_models' not in globals() else trained_models\n",
    "    trained_models['white_bars'] = white_bars_model\n",
    "else:\n",
    "    print('White bars dataset not found. Skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ead9d0",
   "metadata": {},
   "source": [
    "## Train Model for Transparent Black Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7faa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for transparent black detection\n",
    "transparent_black_dataset = datasets.get('transparent_black')\n",
    "if transparent_black_dataset:\n",
    "    print('Processing transparent black model...')\n",
    "    \n",
    "    # Try to load existing model first\n",
    "    transparent_black_model = load_existing_model('transparent_black')\n",
    "    \n",
    "    # If no model was found, train a new one\n",
    "    if transparent_black_model is None:\n",
    "        print('Training new model for transparent black...')\n",
    "        transparent_black_model = create_model()\n",
    "        transparent_black_model = train_model(transparent_black_model, transparent_black_dataset, 'transparent_black')\n",
    "    else:\n",
    "        print('Using existing transparent black model')\n",
    "        black_bars_model = train_model(black_bars_model, black_bars_dataset, 'black_bars', epochs=10)\n",
    "    \n",
    "    # Store in trained_models dictionary\n",
    "    trained_models = {} if 'trained_models' not in globals() else trained_models\n",
    "    trained_models['transparent_black'] = transparent_black_model\n",
    "else:\n",
    "    print('transparent_black dataset not found. Skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad39e71",
   "metadata": {},
   "source": [
    "## Load the Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7bbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all saved models\n",
    "loaded_models = {}\n",
    "\n",
    "for ctype in censorship_types:\n",
    "    model_path = f'pretrained/best_{ctype}_model.pth'\n",
    "    if os.path.exists(model_path):\n",
    "        model = create_model()\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        loaded_models[ctype] = model\n",
    "        print(f\"Loaded model for {ctype}\")\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "    \n",
    "    # Retain the original color image\n",
    "    original_color_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert to grayscale for the model\n",
    "    if len(image.shape) == 3:  # Check if the image has 3 channels (colored)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)  # Convert back to 3 channels for the model\n",
    "    \n",
    "    resize = A.Resize(IMAGE_SIZE, IMAGE_SIZE, interpolation=cv2.INTER_NEAREST)\n",
    "    resized = resize(image=image)[\"image\"]\n",
    "    normalized = A.Normalize()(image=resized)[\"image\"]\n",
    "    tensor_image = ToTensorV2()(image=normalized)[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "    return original_color_image, tensor_image\n",
    "\n",
    "def predict_mask(tensor_image, model):\n",
    "    \"\"\"Predict a mask using the given model.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        prediction = model(tensor_image)\n",
    "        return prediction.squeeze().sigmoid().cpu().numpy()\n",
    "\n",
    "def create_opacity_mask(predicted_mask):\n",
    "    \"\"\"\n",
    "    Create an opacity mask based on thresholds.\n",
    "\n",
    "    Args:\n",
    "        predicted_mask: The predicted mask from the model.\n",
    "\n",
    "    Returns:\n",
    "        The opacity mask.\n",
    "    \"\"\"\n",
    "    opacity_mask = np.zeros_like(predicted_mask, dtype=np.uint8)\n",
    "    opacity_mask[predicted_mask > 0.2] = 50\n",
    "    opacity_mask[predicted_mask > 0.35] = 75\n",
    "    opacity_mask[predicted_mask > 0.5] = 100\n",
    "    return opacity_mask\n",
    "\n",
    "def test_specific_model(test_dir, model_type=None):\n",
    "    \"\"\"\n",
    "    Test specific models based on subdirectories in the test directory.\n",
    "\n",
    "    Args:\n",
    "        test_dir: Path to the test directory containing subdirectories for each censorship type.\n",
    "        model_type: Specific model type to test (e.g., 'white_bars'). If None, test all available models.\n",
    "    \"\"\"\n",
    "    for ctype, model in loaded_models.items():\n",
    "        if model_type and ctype != model_type:\n",
    "            continue  # Skip other models if a specific model_type is provided\n",
    "\n",
    "        specific_test_dir = os.path.join(test_dir, ctype)\n",
    "        if not os.path.exists(specific_test_dir):\n",
    "            print(f\"Skipping {ctype}: No directory found at {specific_test_dir}\")\n",
    "            continue\n",
    "\n",
    "        test_images = [os.path.join(specific_test_dir, filename) for filename in os.listdir(specific_test_dir)\n",
    "                       if filename.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        if not test_images:\n",
    "            print(f\"No test images found in {specific_test_dir} for {ctype}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTesting {ctype} model on {len(test_images)} images in {specific_test_dir}\")\n",
    "\n",
    "        for img_path in test_images:\n",
    "            try:\n",
    "                print(f\"Testing on: {img_path}\")\n",
    "                image, tensor_image = preprocess_image(img_path)\n",
    "                predicted_mask = predict_mask(tensor_image, model)\n",
    "                opacity_mask = create_opacity_mask(predicted_mask)\n",
    "\n",
    "                # Display results\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title(\"Input Image\")\n",
    "\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(opacity_mask, cmap=\"gray\")\n",
    "                plt.title(f\"{ctype} Detection\")\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "def export_image_and_mask(image, mask, output_dir, name):\n",
    "    \"\"\"Save an image or mask to the specified directory.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"{name}.png\")\n",
    "    if len(mask.shape) == 2:  # Mask\n",
    "        Image.fromarray(mask).save(output_path, format=\"PNG\")\n",
    "    else:  # Image\n",
    "        Image.fromarray(image).save(output_path, format=\"PNG\")\n",
    "\n",
    "def export_for_lama_inpainting(model_types=None):\n",
    "    \"\"\"\n",
    "    Process test images with selected models and save results for LAMA inpainting.\n",
    "\n",
    "    Args:\n",
    "        model_types: List of censorship types to use (e.g., ['black_bars', 'white_bars']).\n",
    "                     If None, all available models will be used.\n",
    "    \"\"\"\n",
    "    base_test_dir = 'input'\n",
    "    demo_input_dir = '../lama-inpainting/input/images'\n",
    "    demo_mask_dir = '../lama-inpainting/input/masks'\n",
    "\n",
    "    if model_types is None:\n",
    "        model_types = list(loaded_models.keys())\n",
    "    model_types = [t for t in model_types if t in loaded_models]\n",
    "\n",
    "    if not model_types:\n",
    "        print(\"No valid model types specified!\")\n",
    "        return\n",
    "\n",
    "    print(f\"Using models: {', '.join(model_types)}\")\n",
    "\n",
    "    for ctype in model_types:\n",
    "        specific_test_dir = os.path.join(base_test_dir, ctype)\n",
    "        if not os.path.exists(specific_test_dir):\n",
    "            print(f\"Skipping {ctype}: No directory found at {specific_test_dir}\")\n",
    "            continue\n",
    "\n",
    "        test_images = [os.path.join(specific_test_dir, filename) for filename in os.listdir(specific_test_dir)\n",
    "                       if filename.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        print(f\"Found {len(test_images)} test images for {ctype}\")\n",
    "\n",
    "        for img_path in test_images:\n",
    "            try:\n",
    "                filename = os.path.basename(img_path)\n",
    "                name, _ = os.path.splitext(filename)\n",
    "\n",
    "                # Preprocess image\n",
    "                original_color_image, tensor_image = preprocess_image(img_path)\n",
    "\n",
    "                # Predict mask using the model\n",
    "                model = loaded_models[ctype]\n",
    "                predicted_mask = predict_mask(tensor_image, model)\n",
    "                opacity_mask = create_opacity_mask(predicted_mask)\n",
    "\n",
    "                # Normalize mask to 0-255 range\n",
    "                if opacity_mask.max() > 0:\n",
    "                    opacity_mask = (opacity_mask / opacity_mask.max() * 255).astype(np.uint8)\n",
    "\n",
    "                # Resize mask back to original image size\n",
    "                h, w = original_color_image.shape[:2]\n",
    "                resized_mask = cv2.resize(opacity_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                # Save original color image and mask\n",
    "                export_image_and_mask(original_color_image, original_color_image, demo_input_dir, name)\n",
    "                export_image_and_mask(resized_mask, resized_mask, demo_mask_dir, name)\n",
    "\n",
    "                print(f\"Processed {filename} -> {name}.png\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    print(f\"Exported images and masks to:\")\n",
    "    print(f\"  Images: {demo_input_dir}\")\n",
    "    print(f\"  Masks: {demo_mask_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443b9c9",
   "metadata": {},
   "source": [
    "## Test the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_specific_model(test_dir=\"input\", model_type=\"black_bars\")\n",
    "# test_specific_model(test_dir=\"input\", model_type=\"white_bars\")\n",
    "# test_specific_model(test_dir=\"input\", model_type=\"transparent_black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf3630",
   "metadata": {},
   "source": [
    "## Export Images and Masks for LAMA Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7558eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_for_lama_inpainting(model_types=[\"black_bars\", \"white_bars\"])\n",
    "export_for_lama_inpainting(model_types=[\"transparent_black\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
